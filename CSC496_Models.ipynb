{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIefZT3rIgjRXts65qP/xM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anfalaugust/CSC496_Models/blob/main/CSC496_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "73FwZd0dxcZc",
        "outputId": "56c186e7-2d93-4c1f-becc-6c7f89b6af8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/CSC496/Diseases of date palm leaves dataset.zip\n",
            "replace /content/dataset/Diseases of date palm leaves dataset/Infected Date Palm Leaves Dataset/Processed/1. Potassium Deficiency/M9 (101).png? [y]es, [n]o, [A]ll, [N]one, [r]ename: Found 3089 files belonging to 9 classes.\n",
            "Classes: ['1. Potassium Deficiency', '2. Manganese Deficiency', '3. Magnesium Deficiency', '4. Black Scorch', '5. Leaf Spots', '6. Fusarium Wilt', '7. Rachis Blight', '8. Parlatoria Blanchardi', '9. Healthy sample']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.listdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/CSC496/Diseases of date palm leaves dataset.zip\" -d \"/content/dataset\"\n",
        "\n",
        "import tensorflow as tf\n",
        "os.listdir(\"/content/dataset\")\n",
        "\n",
        "dataset_path = \"/content/dataset/Diseases of date palm leaves dataset/Infected Date Palm Leaves Dataset/Processed\"\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(\"Classes:\", train_ds.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J4115nt7x5JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# ResNet50: Load Data -> Train -> Evaluate (One Cell)\n",
        "# ==============================\n",
        "\n",
        "\n",
        "# 1) Paths & Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "EPOCHS = 10\n",
        "\n",
        "# 2) Load dataset + Train/Validation split (80/20)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 3) Optimize pipeline\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 4) Build Model (Transfer Learning)\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Rescaling(1./255),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# 5) Compile\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6) Train\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "# 7) Evaluate\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(\"✅ Validation Accuracy:\", val_acc)\n",
        "print(\"✅ Validation Loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mnjVwK-Dzg6S",
        "outputId": "83e05842-9028-4004-d6df-7b8f231d278b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3089 files belonging to 9 classes.\n",
            "Using 2472 files for training.\n",
            "Found 3089 files belonging to 9 classes.\n",
            "Using 617 files for validation.\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 249ms/step - accuracy: 0.2179 - loss: 2.2372 - val_accuracy: 0.2480 - val_loss: 1.9375\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 110ms/step - accuracy: 0.2658 - loss: 1.9798 - val_accuracy: 0.2496 - val_loss: 1.8968\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.2883 - loss: 1.9093 - val_accuracy: 0.3209 - val_loss: 1.8567\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.2927 - loss: 1.8789 - val_accuracy: 0.3193 - val_loss: 1.8499\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.3065 - loss: 1.8785 - val_accuracy: 0.2966 - val_loss: 1.8357\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.3040 - loss: 1.8803 - val_accuracy: 0.3015 - val_loss: 1.8113\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.3070 - loss: 1.8438 - val_accuracy: 0.3306 - val_loss: 1.7901\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.3111 - loss: 1.8505 - val_accuracy: 0.3647 - val_loss: 1.7777\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.3290 - loss: 1.8208 - val_accuracy: 0.3339 - val_loss: 1.7735\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.3220 - loss: 1.8169 - val_accuracy: 0.3371 - val_loss: 1.7664\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.3569 - loss: 1.7494\n",
            "✅ Validation Accuracy: 0.337115079164505\n",
            "✅ Validation Loss: 1.7664309740066528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# MobileNetV2: Load Data -> Train -> Evaluate (One Cell)\n",
        "# ==============================\n",
        "\n",
        "# 1) Paths & Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "EPOCHS = 10\n",
        "\n",
        "# 2) Load dataset + Train/Validation split (80/20)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 3) Optimize pipeline\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 4) Build Model (Transfer Learning)\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Rescaling(1./255),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# 5) Compile\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6) Train\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "# 7) Evaluate\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(\"✅ Validation Accuracy:\", val_acc)\n",
        "print(\"✅ Validation Loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pkJHy3471Ssu",
        "outputId": "79465cc6-7ce3-49ad-e21d-19358dff3bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3089 files belonging to 9 classes.\n",
            "Using 2472 files for training.\n",
            "Found 3089 files belonging to 9 classes.\n",
            "Using 617 files for validation.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 403ms/step - accuracy: 0.3408 - loss: 1.9836 - val_accuracy: 0.6872 - val_loss: 0.9837\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6272 - loss: 1.1056 - val_accuracy: 0.7407 - val_loss: 0.7585\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7042 - loss: 0.8792 - val_accuracy: 0.8055 - val_loss: 0.6382\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.7871 - loss: 0.6434 - val_accuracy: 0.8331 - val_loss: 0.5348\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8172 - loss: 0.5581 - val_accuracy: 0.8363 - val_loss: 0.5197\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8411 - loss: 0.5166 - val_accuracy: 0.8703 - val_loss: 0.4446\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8479 - loss: 0.4431 - val_accuracy: 0.8768 - val_loss: 0.4093\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8845 - loss: 0.3658 - val_accuracy: 0.8865 - val_loss: 0.3951\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8908 - loss: 0.3417 - val_accuracy: 0.8995 - val_loss: 0.3456\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9285 - loss: 0.2425 - val_accuracy: 0.8979 - val_loss: 0.3390\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9173 - loss: 0.3085\n",
            "✅ Validation Accuracy: 0.8978930115699768\n",
            "✅ Validation Loss: 0.33900052309036255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Hybrid EfficientNet + Transformer\n",
        "# ==============================\n",
        "\n",
        "IMG_SIZE = (300, 300)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "EPOCHS = 20\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)\n",
        "val_ds   = val_ds.cache().prefetch(AUTOTUNE)\n",
        "\n",
        "base_model = tf.keras.applications.EfficientNetB3(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(300,300,3)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=(300,300,3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.Reshape((-1, x.shape[-1]))(x)\n",
        "\n",
        "# Transformer block\n",
        "attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
        "x = layers.Add()([x, attention])\n",
        "x = layers.LayerNormalization()(x)\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(\"✅ Validation Accuracy:\", val_acc)\n",
        "print(\"✅ Validation Loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev_BteYn3kKr",
        "outputId": "c864a1c2-6723-466c-e73e-8a4d4817cb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3089 files belonging to 9 classes.\n",
            "Using 2472 files for training.\n",
            "Found 3089 files belonging to 9 classes.\n",
            "Using 617 files for validation.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 988ms/step - accuracy: 0.2191 - loss: 2.1774 - val_accuracy: 0.2480 - val_loss: 1.9289\n",
            "Epoch 2/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 164ms/step - accuracy: 0.2495 - loss: 2.0142 - val_accuracy: 0.2545 - val_loss: 1.9387\n",
            "Epoch 3/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.2453 - loss: 1.9738 - val_accuracy: 0.2853 - val_loss: 1.9097\n",
            "Epoch 4/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.2464 - loss: 1.9772 - val_accuracy: 0.2480 - val_loss: 1.8922\n",
            "Epoch 5/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 153ms/step - accuracy: 0.2706 - loss: 1.9618 - val_accuracy: 0.2480 - val_loss: 1.8710\n",
            "Epoch 6/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 154ms/step - accuracy: 0.2442 - loss: 1.9370 - val_accuracy: 0.3031 - val_loss: 1.8857\n",
            "Epoch 7/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - accuracy: 0.2441 - loss: 1.9656 - val_accuracy: 0.2917 - val_loss: 1.8571\n",
            "Epoch 8/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.2586 - loss: 1.9419 - val_accuracy: 0.2885 - val_loss: 1.8710\n",
            "Epoch 9/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.2655 - loss: 1.9349 - val_accuracy: 0.3047 - val_loss: 1.8648\n",
            "Epoch 10/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.2490 - loss: 1.9243 - val_accuracy: 0.3015 - val_loss: 1.8618\n",
            "Epoch 11/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - accuracy: 0.2805 - loss: 1.9326 - val_accuracy: 0.2836 - val_loss: 1.8626\n",
            "Epoch 12/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - accuracy: 0.2820 - loss: 1.9074 - val_accuracy: 0.2934 - val_loss: 1.8752\n",
            "Epoch 13/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.2654 - loss: 1.9395 - val_accuracy: 0.3063 - val_loss: 1.8345\n",
            "Epoch 14/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.2792 - loss: 1.9241 - val_accuracy: 0.3096 - val_loss: 1.8828\n",
            "Epoch 15/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - accuracy: 0.2810 - loss: 1.9287 - val_accuracy: 0.2934 - val_loss: 1.8484\n",
            "Epoch 16/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 163ms/step - accuracy: 0.2520 - loss: 1.9498 - val_accuracy: 0.2885 - val_loss: 1.8809\n",
            "Epoch 17/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 165ms/step - accuracy: 0.2628 - loss: 1.9172 - val_accuracy: 0.2917 - val_loss: 1.8568\n",
            "Epoch 18/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.2691 - loss: 1.9358 - val_accuracy: 0.2950 - val_loss: 1.8612\n",
            "Epoch 19/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 155ms/step - accuracy: 0.2802 - loss: 1.9236 - val_accuracy: 0.2771 - val_loss: 1.8464\n",
            "Epoch 20/20\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 155ms/step - accuracy: 0.2685 - loss: 1.9116 - val_accuracy: 0.2480 - val_loss: 1.8511\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.2640 - loss: 1.8356\n",
            "✅ Validation Accuracy: 0.24797406792640686\n",
            "✅ Validation Loss: 1.8511375188827515\n"
          ]
        }
      ]
    }
  ]
}